{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from neural_network import NeuralNetwork\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.models import Sequential\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_item_records_from_summary():\n",
    "    \"\"\" Generate item records using the summary.json link from the OSBuddy API \"\"\"\n",
    "\n",
    "    df = pd.read_json(path_or_buf='https://rsbuddy.com/exchange/summary.json',orient='index', convert_axes=True)\n",
    "    df = df[['id','name','buy_average','buy_quantity','sell_average','sell_quantity','overall_average','overall_quantity']]\n",
    "    data = df.sort_values(by=['id']).reset_index()\n",
    "    data = data.drop(labels='index',axis=1)\n",
    "\n",
    "    #Output item id/name pairs to a csv file\n",
    "    item_key = data[['id', 'name']]\n",
    "    file_name = './item_key.csv'\n",
    "    item_key.to_csv(path_or_buf=file_name, columns=('id','name'), index=False)\n",
    "\n",
    "\n",
    "def generate_input_data_by_item_number(start_num, num_items): \n",
    "    \"\"\"Pull items by range of Index values\"\"\"\n",
    "    file_name = './item_key.csv'\n",
    "    all_items = pd.read_csv(file_name, skiprows=[])\n",
    "    items = all_items[start_num: start_num + num_items]\n",
    "    print(\"Item List generated by item number:\")\n",
    "    print(items)\n",
    "    return items\n",
    "\n",
    "def generate_input_data_by_item_name(names):  #Pull a specific items by name\n",
    "    \"\"\"Pull specific items by name\"\"\"\n",
    "    file_name = './item_key.csv'\n",
    "    data = pd.read_csv(file_name, skiprows=[])\n",
    "    items = pd.DataFrame()\n",
    "    for name in names:\n",
    "        items = items.append(data.loc[data['name'] == name])\n",
    "    items = items.reset_index().drop(labels='index',axis=1)\n",
    "    print(\"Item List generated by name:\")\n",
    "    print(items)\n",
    "    return items\n",
    "\n",
    "def get_item_records_from_url(input_data):\n",
    "    \"\"\" Iterate through all items, grab data from api and append to dataframe \"\"\"\n",
    "    i = 0\n",
    "    item_lookup_failed = False\n",
    "    item_records = pd.DataFrame(columns=['id', 'name', 'data'])\n",
    "    while(i<input_data['id'].count()):\n",
    "        key = input_data.iloc[i]['id']\n",
    "        name = input_data.iloc[i]['name']\n",
    "        #Print item information only on first data retrieval attempt \n",
    "        if not item_lookup_failed:\n",
    "            print('Querying API for ' + name + ' data. ' + 'Item Id: ' + str(key))\n",
    "        try:\n",
    "            #Attempt to get data from API, retry if HTTP error\n",
    "            url = f'https://api.rsbuddy.com/grandExchange?a=graph&g=240&start=1474615279000&i={key}'\n",
    "            temp_df = pd.DataFrame()\n",
    "            temp_df = pd.read_json(path_or_buf=url,orient='records', convert_axes=False)\n",
    "            item_records = item_records.append({'id':key, 'name': name, 'data':temp_df}, ignore_index=True)\n",
    "            i+=1\n",
    "            item_lookup_failed = False\n",
    "        except:\n",
    "            print(\"Retrying...\")\n",
    "            time.sleep(1) #Avoid getting blacklisted by API\n",
    "            item_lookup_failed = True\n",
    "    print('Item retrieval complete!')\n",
    "\n",
    "    #Add correct formatting to item record dates/times\n",
    "    item_records = format_item_record_dates(item_records)\n",
    "\n",
    "    return item_records\n",
    "\n",
    "def format_item_record_dates(item_records):\n",
    "    \"\"\" Converts timestamp to Unix seconds, tacks on formatted date field, and creates Unix seconds from most recent datapoint, and Unix seconds position from Jan-1\"\"\"\n",
    "    #Iterate through each item and convert timestamps from milliseconds to seconds\n",
    "    for _, row in item_records.iterrows():\n",
    "        print(\"Formatting item record dates for: \" +  row['name'] + \"...\")\n",
    "        row['data'] = row['data'][['ts','buyingPrice','buyingCompleted','sellingPrice','sellingCompleted', 'overallPrice','overallCompleted']]\n",
    "        row['data'] = row['data'].sort_values(by=['ts'],ascending=1).reset_index()\n",
    "        for ind, r in row['data'].iterrows():\n",
    "            r['ts'] = int(r['ts']/1000)\n",
    "            row['data'].loc[ind, 'ts'] = int(r['ts'])\n",
    "            \n",
    "        #Refactor unix timestamps where the most recent record is 0\n",
    "        timestamps = pd.DataFrame(columns=['ts','tsFromCurrent', 'tsYtd'],data=row['data']['ts'])\n",
    "        lastTs = timestamps['ts'][len(timestamps)-1]\n",
    "        i = 0\n",
    "        print ('Finding timestamps from most recent datapoint.')\n",
    "        while(i<len(timestamps)):\n",
    "            timestamps['tsFromCurrent'][i] = abs(timestamps['ts'][i] - lastTs)    \n",
    "            i += 1\n",
    "        \n",
    "        #Create a list of all years and corresponding unix since 1970 (Unix = 0)\n",
    "        years = pd.DataFrame(columns=['year', 'unix'])    \n",
    "        i = 0\n",
    "        for year in range(2015,2050):\n",
    "            unix = (year - 1970) * 31557600\n",
    "            years.loc[i] = [year,unix]\n",
    "            i+=1\n",
    "\n",
    "        #Refactor unix timestamps to show time since Jan-1\n",
    "        i = 0\n",
    "        j = 0\n",
    "        newYear = True\n",
    "        while(i<len(timestamps)):\n",
    "            #Iterate through each year to find all timestamps wihtin that date range\n",
    "            unixYear = years.iloc[j]['unix']\n",
    "            if newYear == True:\n",
    "                print(\"Checking entries for year: \" + str(years.iloc[j]['year']))\n",
    "            ytd = abs(timestamps['ts'][i] - unixYear)\n",
    "            if(ytd < 31557600):\n",
    "                ytd = abs(timestamps.iloc[i]['ts'] - unixYear) #this may be redundant. possibly deprecate\n",
    "                timestamps['tsYtd'][i] = int(ytd)\n",
    "                i += 1\n",
    "                newYear = False\n",
    "            else: \n",
    "                print(\"Done with this year. Moving on to the next.\")\n",
    "                j += 1\n",
    "                newYear = True\n",
    "        #Append the new timestamps to the master item DataFrame\n",
    "        for ind, r in row['data'].iterrows():\n",
    "            row['data'].loc[ind, 'date'] = datetime.datetime.fromtimestamp(r['ts']).isoformat()\n",
    "            row['data'].loc[ind, 'tsFromCurrent'] = timestamps.iloc[ind]['tsFromCurrent']\n",
    "            row['data'].loc[ind, 'tsYtd'] = timestamps.iloc[ind]['tsYtd']            \n",
    "        row['data'] = row['data'].drop(labels='index',axis=1)\n",
    "    return item_records\n",
    "\n",
    "def generate_train_and_test_deltas(item_records):\n",
    "    \"\"\" \n",
    "        Iterate through each item record and generate percentage deltas for buy price, \n",
    "        buy completed, sell price, sell completed, overall price, and overall completed\n",
    "        compared to last sample period. \n",
    "    \"\"\"\n",
    "    for _, row in item_records.iterrows():\n",
    "        for col_name in ['buyingPricePer', 'buyingCompletedPer', 'sellingPricePer', \n",
    "                        'sellingCompletedPer', 'overallPricePer', 'overallCompletedPer']:\n",
    "            series_length = len(row['data']['ts'])\n",
    "            row['data'][col_name] = pd.Series(np.random.randn(series_length), index=row['data'].index)\n",
    "        #Each row represents and item, which has an associated dataframe \n",
    "        for index, _ in row['data'].iterrows():\n",
    "            #Use data from last row to calc percentage change\n",
    "            for col_name in ['buyingPrice', 'buyingCompleted', 'sellingPrice', 'sellingCompleted', \n",
    "                            'overallPrice', 'overallCompleted']:\n",
    "                if(index == 0):\n",
    "                    row['data'].loc[index, col_name + 'Per'] = 0\n",
    "                else:\n",
    "                    row['data'].loc[index, col_name + 'Per'] = calculate_percentage_change(row['data'], col_name, index)\n",
    "                \n",
    "                \n",
    "            \n",
    "def calculate_percentage_change(dataframe, columnName, index):\n",
    "    \"\"\"Calculate delta from last time stamp\"\"\"\n",
    "    return (dataframe.loc[index, columnName] - dataframe.loc[index-1, columnName])/dataframe.loc[index-1, columnName]\n",
    "\n",
    "\n",
    "def createTrainAndTestSet(item_records, start_stamp, epoch_size, num_steps_ahead_to_predict):\n",
    "    print(\"Creating Test and Training Datasets...\")\n",
    "    #create test_item object to log training and test sets\n",
    "    test_item = pd.DataFrame(columns=['id', 'name', 'train_data', 'test_data', 'pred_data'])\n",
    "    record_index = 0\n",
    "    end_stamp = start_stamp + math.ceil(epoch_size*6)\n",
    "    \n",
    "    #iterate through the urlquery results to generate training sets\n",
    "    while(record_index < item_records['id'].count()):  \n",
    "        item_id = item_records.iloc[record_index]['id'] \n",
    "        print(\"Creating Datasets for item ID: \" + str(item_id))\n",
    "        \n",
    "        #takes the item_id and generates the test data for the specified parameters as an array\n",
    "        test_set = item_records.loc[item_records['id'] == item_id]\n",
    "        \n",
    "        #for column key, uncomment this line below:\n",
    "        #train_x_headers = list(urlquery.iloc[0]['data'].columns.values[0:6])\n",
    "        #print(train_x_headers)\n",
    "\n",
    "        \n",
    "        \n",
    "        #Create each epoch of train/test data\n",
    "        predict_stamp = end_stamp + num_steps_ahead_to_predict\n",
    "        #Ensure that the value we are attempting to predict exists for testing purposes\n",
    "        if predict_stamp < len(test_set.iloc[0]['data']):\n",
    "            #create training datasets\n",
    "            train_data = test_set.iloc[0]['data'].iloc[start_stamp:end_stamp].values[:,8:15]\n",
    "\n",
    "            #create test set to estimate the price six periods from now (24 hours) \n",
    "            price_col = 14\n",
    "            test_data = test_set.iloc[0]['data'].iloc[end_stamp+num_pred_vals][price_col]\n",
    "\n",
    "           \n",
    "\n",
    "            #test key to compare to Y-hat\n",
    "            test_item = test_item.append({'id':item_id, 'name': test_set.iloc[0]['name'], 'train_data': train_data, 'test_data': test_data}, ignore_index=True)\n",
    "            print(\"Record completed at test_item position [\" + str(record_index) + \"]\")\n",
    "            \n",
    "        record_index += 1\n",
    "\n",
    "    return test_item\n",
    "\n",
    "    #TODO: Test model size\n",
    "\n",
    "\n",
    "    #random shit\n",
    "    #import keras.backend as K\n",
    "    \"\"\"\n",
    "    train_val = test_item.iloc[2]['train_val'] # test data. not real\n",
    "    train_y = test_item.iloc[2]['train_y'] # test data. not real\n",
    "\n",
    "    \n",
    "    def create_model(layers,activation):\n",
    "        model = Sequential()\n",
    "        for i, nodes in enumerate(layers):\n",
    "            if i==0:\n",
    "                model.add(Dense(nodes,input_dim=train_val.shape[1]))\n",
    "                model.add(Activation(activation))\n",
    "            else: \n",
    "                model.add(Dense(nodes))\n",
    "                model.add(Activation(activation))\n",
    "        model.add(Dense(1)) #Note: no activations present beyond this point\n",
    "\n",
    "        model.compile(optimizer='adadelta', loss='mse')\n",
    "        return model\n",
    "\n",
    "    print('creating model')\n",
    "    model = KerasRegressor(build_fn=create_model, verbose = 0)\n",
    "    layers = [[16], [4,2], [4], [16,4]]\n",
    "    activations = ['tanh', 'relu']\n",
    "    param_grid = dict(layers=layers, activation=activations, batch_size = [42, 180], epochs=[6])\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error')\n",
    "\n",
    "    # grid_result = grid.fit(train_x, train_y) testing for network size \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item List generated by name:\n",
      "      id          name\n",
      "0   1741       Leather\n",
      "1  11840  Dragon boots\n",
      "2    892    Rune arrow\n",
      "Querying API for Leather data. Item Id: 1741\n",
      "Retrying...\n",
      "Retrying...\n",
      "Retrying...\n",
      "Retrying...\n",
      "Querying API for Dragon boots data. Item Id: 11840\n",
      "Querying API for Rune arrow data. Item Id: 892\n",
      "Retrying...\n",
      "Item retrieval complete!\n",
      "Formatting item record dates for: Leather...\n",
      "Finding timestamps from most recent datapoint.\n",
      "Checking entries for year: 2015\n",
      "Done with this year. Moving on to the next.\n",
      "Checking entries for year: 2016\n",
      "Done with this year. Moving on to the next.\n",
      "Checking entries for year: 2017\n",
      "Done with this year. Moving on to the next.\n",
      "Checking entries for year: 2018\n",
      "Formatting item record dates for: Dragon boots...\n",
      "Finding timestamps from most recent datapoint.\n",
      "Checking entries for year: 2015\n",
      "Done with this year. Moving on to the next.\n",
      "Checking entries for year: 2016\n",
      "Done with this year. Moving on to the next.\n",
      "Checking entries for year: 2017\n",
      "Done with this year. Moving on to the next.\n",
      "Checking entries for year: 2018\n",
      "Formatting item record dates for: Rune arrow...\n",
      "Finding timestamps from most recent datapoint.\n",
      "Checking entries for year: 2015\n",
      "Done with this year. Moving on to the next.\n",
      "Checking entries for year: 2016\n",
      "Done with this year. Moving on to the next.\n",
      "Checking entries for year: 2017\n",
      "Done with this year. Moving on to the next.\n",
      "Checking entries for year: 2018\n",
      "Creating Test and Training Datasets...\n",
      "Creating Datasets for item ID: 1741\n",
      "Record completed at test_item position [0]\n",
      "Creating Datasets for item ID: 11840\n",
      "Record completed at test_item position [1]\n",
      "Creating Datasets for item ID: 892\n",
      "Record completed at test_item position [2]\n",
      "Data for Epoch 0\n",
      "-----------------\n",
      "      id          name                                         train_data  \\\n",
      "0   1741       Leather  [[60782400.0, 22953600.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "1  11840  Dragon boots  [[60782400.0, 22953600.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2    892    Rune arrow  [[60782400.0, 22953600.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "   test_data  pred_data  \n",
      "0   0.042553        NaN  \n",
      "1   0.010881        NaN  \n",
      "2   0.000000        NaN  \n",
      "Creating Test and Training Datasets...\n",
      "Creating Datasets for item ID: 1741\n",
      "Record completed at test_item position [0]\n",
      "Creating Datasets for item ID: 11840\n",
      "Record completed at test_item position [1]\n",
      "Creating Datasets for item ID: 892\n",
      "Record completed at test_item position [2]\n",
      "Data for Epoch 1\n",
      "-----------------\n",
      "      id          name                                         train_data  \\\n",
      "0   1741       Leather  [[60782400.0, 22953600.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "1  11840  Dragon boots  [[60782400.0, 22953600.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2    892    Rune arrow  [[60782400.0, 22953600.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "   test_data  pred_data  \n",
      "0   0.042553        NaN  \n",
      "1   0.010881        NaN  \n",
      "2   0.000000        NaN  \n",
      "Creating Test and Training Datasets...\n",
      "Creating Datasets for item ID: 1741\n",
      "Record completed at test_item position [0]\n",
      "Creating Datasets for item ID: 11840\n",
      "Record completed at test_item position [1]\n",
      "Creating Datasets for item ID: 892\n",
      "Record completed at test_item position [2]\n",
      "Data for Epoch 2\n",
      "-----------------\n",
      "      id          name                                         train_data  \\\n",
      "0   1741       Leather  [[60782400.0, 22953600.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "1  11840  Dragon boots  [[60782400.0, 22953600.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2    892    Rune arrow  [[60782400.0, 22953600.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "   test_data  pred_data  \n",
      "0   0.042553        NaN  \n",
      "1   0.010881        NaN  \n",
      "2   0.000000        NaN  \n",
      "32\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #Get a list of all items\n",
    "    generate_item_records_from_summary()\n",
    "\n",
    "    #Pull items by range of Index values\n",
    "    #input_data = generate_input_data_by_item_number(0,3)\n",
    "    \n",
    "    #Pull specific items by name\n",
    "    input_data = generate_input_data_by_item_name(['Leather','Dragon boots', 'Rune arrow'])\n",
    "\n",
    "    #In Days - define the number of epochs to include in the test data\n",
    "    num_pred_vals = 6 #Each value is a 4 hour record\n",
    "    start_stamp = 0\n",
    "    epoch_size = 30\n",
    "    num_epochs = 3\n",
    "    \n",
    "    #Generate item records\n",
    "    item_records = get_item_records_from_url(input_data)\n",
    "    \n",
    "    #Generate deltas for data\n",
    "    generate_train_and_test_deltas(item_records)\n",
    "\n",
    "    #Create test_item object to log training and test sets\n",
    "    test_train_set = pd.DataFrame(columns=['epoch_id', 'item_data'])\n",
    "    \n",
    "    #Calculate number of epochs required then\n",
    "    \n",
    "    #Get item data for each epoch\n",
    "    for i in range(num_epochs):\n",
    "        curr_epoch = pd.DataFrame(columns=['epoch_id', 'item_data'])\n",
    "        test_data = createTrainAndTestSet(item_records, start_stamp, epoch_size, num_pred_vals)\n",
    "        print('Data for Epoch {0}\\n-----------------'.format(i))\n",
    "        print(test_data)\n",
    "        #curr_epoch = curr_epoch.append([i, createTrainAndTestSet(item_records, start_stamp, epoch_size, num_pred_vals)])\n",
    "        #test_train_set = test_train_set.append(curr_epoch)\n",
    "\n",
    "    #Test creation of Neural Net\n",
    "    nn = NeuralNetwork()\n",
    "    nn.generateInitialNetwork([1,1])\n",
    "    nn.calculateNodeActivations()\n",
    "    print(nn.outputNode.getActivation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch_id</th>\n",
       "      <th>item_data</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id          name                        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id          name                        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id          name                        ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  epoch_id item_data                                                  0\n",
       "0      NaN       NaN                                                  0\n",
       "1      NaN       NaN        id          name                        ...\n",
       "0      NaN       NaN                                                  1\n",
       "1      NaN       NaN        id          name                        ...\n",
       "0      NaN       NaN                                                  2\n",
       "1      NaN       NaN        id          name                        ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_records.iloc[0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=\"+3\"><b>Testing of network size:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import keras.backend as K\n",
    "train_x = test_item.iloc[2]['train_x'] # test data. not real\n",
    " \n",
    "def create_model(layesr,activation):\n",
    "    model = Sequential()\n",
    "    for i, nodes in enumerate(layers):\n",
    "        if i==0:\n",
    "            model.add(Dense(nodes,input_dim=train_x.shape[1]))\n",
    "            model.add(Activation(activation))\n",
    "        else: \n",
    "            model.add(Dense(nodes))\n",
    "            model.add(Activation(activation))\n",
    "    model.add(Dense(1)) #Note: no activations present beyond this point\n",
    "\n",
    "    model.compile(optimizer='adadelta', loss='mse')\n",
    "    return model\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = [[16], [4,2], [4], [16,4]]\n",
    "activations = [tanh, relu]\n",
    "param_grid = dict(layers=layers, activation=activations, batch_size = [42, 180], epochs[6])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
