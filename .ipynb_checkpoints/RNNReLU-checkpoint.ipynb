{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from node import Node\n",
    "from edge import Edge\n",
    "from neural_network import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://rsbuddy.com/exchange/summary.json'\n",
    "urljson = urllib.request.urlopen(url)\n",
    "jsondata = json.loads(urljson.read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get a list of all items\n",
    "df = pd.read_json(path_or_buf='https://rsbuddy.com/exchange/summary.json',orient='index', convert_axes=True)\n",
    "df = df[['id','name','buy_average','buy_quantity','sell_average','sell_quantity','overall_average','overall_quantity']]\n",
    "data = df.sort_values(by=['id']).reset_index()\n",
    "data = data.drop(labels='index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run this to populate a csv: itemKey\n",
    "itemKey = data[['id', 'name']]\n",
    "itemKey.to_csv(path_or_buf='./itemKey.csv', columns=('id','name'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create test set of specified range number of items\n",
    "data = pd.read_csv('./itemKey.csv', skiprows=[])\n",
    "startNum = 0\n",
    "numItems = 3\n",
    "inputdata = data[startNum:startNum+numItems]\n",
    "inputdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this to pull a specific items by name\n",
    "names = ['Armadyl godsword','Fire rune', 'Soft clay']\n",
    "\n",
    "#read the itemKey csv and parse the key for associated item ids\n",
    "data = pd.read_csv('./itemKey.csv', skiprows=[])\n",
    "inputdata = pd.DataFrame()\n",
    "for name in names:\n",
    "    inputdata = inputdata.append(data.loc[data['name'] == name])\n",
    "inputdata = inputdata.reset_index().drop(labels='index',axis=1)\n",
    "\n",
    "print(\"Query List:\")\n",
    "print(inputdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "firtIteration = True\n",
    "urlquery = pd.DataFrame(columns=['id', 'name', 'data'])\n",
    "#Iterate through all items, grab data from api and append to urlquery dataframe\n",
    "while(i<inputdata['id'].count()):\n",
    "    key = inputdata.iloc[i]['id']\n",
    "    name = inputdata.iloc[i]['name']\n",
    "    if (firtIteration == True):\n",
    "        print(\"Item Id - \" + str(key) + \": \" + name)\n",
    "    #Attempt to get data from API, retry if HTTP error\n",
    "    try:\n",
    "        url = f'https://api.rsbuddy.com/grandExchange?a=graph&g=240&start=1474615279000&i={key}'\n",
    "        tempDataFrame = pd.DataFrame()\n",
    "        tempDataFrame = pd.read_json(path_or_buf=url,orient='records', convert_axes=False)\n",
    "        urlquery = urlquery.append({'id':key, 'name': name, 'data':tempDataFrame}, ignore_index=True)\n",
    "        i+=1\n",
    "        #check if there are more items to run. If no more items, return message indicating the process is done\n",
    "        if (i == inputdata['id'].count()):\n",
    "            print(\"It worked! That was the last item!\")\n",
    "        else:\n",
    "            print(\"It worked! On to the next one.\")\n",
    "        firtIteration = True\n",
    "    except:\n",
    "        print(\"Got fucked. Trying again. :)\")\n",
    "        time.sleep(1)\n",
    "        firtIteration = False\n",
    "#Reformat urlquery columns, convert timestamp from miliseconds to seconds, convert timestamp to date, and sort by date\n",
    "for index, row in urlquery.iterrows():\n",
    "    row['data'] = row['data'][['ts','buyingPrice','buyingCompleted','sellingPrice','sellingCompleted','overallPrice','overallCompleted']]\n",
    "    row['data'] = row['data'].sort_values(by=['ts'],ascending=1).reset_index()\n",
    "    count = 0 \n",
    "    for ind, r in row['data'].iterrows():\n",
    "        r['ts'] = r['ts']/1000\n",
    "        row['data'].loc[ind, 'ts'] = int(r['ts'])\n",
    "        row['data'].loc[ind, 'date'] = datetime.datetime.fromtimestamp(r['ts']).isoformat()\n",
    "    row['data'] = row['data'].drop(labels='index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_size = 360 #In Days - define the number of epochs to include in the test data\n",
    "\n",
    "#create test_item object to log training and test sets\n",
    "test_item = pd.DataFrame(columns=['id', 'name', 'train_x', 'train_y', 'test_x', 'test_y', 'pred_y'])\n",
    "h = 0\n",
    "#iterate through the urlquery results to generate training sets\n",
    "while(h<urlquery['id'].count()):  \n",
    "    item_id = urlquery.iloc[h]['id'] #Define the item to be queried for\n",
    "    print(\"Item ID: \" + str(item_id))\n",
    "    #takes the item_id and generates the test data for the specified parameters as an array\n",
    "    test_set = urlquery.loc[urlquery['id'] == item_id]\n",
    "    train_x_headers = list(urlquery.iloc[0]['data'].columns.values[0:5])\n",
    "    \n",
    "    #for column key, uncomment this line below:\n",
    "    #print(train_x_headers)\n",
    "\n",
    "    #create training datasets\n",
    "    train_x = test_set.iloc[0]['data'].iloc[0:math.ceil(test_set_size*6)].values[:,0:5]\n",
    "    train_y = test_set.iloc[0]['data'].iloc[0:math.ceil(test_set_size*6)].values[:,5]\n",
    "    print (\"The shape of the \" + test_set.iloc[0]['name'] +  \" input array is: \"+ str(train_x.shape))\n",
    "\n",
    "    #create test set to estimate next 6 prices (next day of prices)\n",
    "    test_x = test_set.iloc[0]['data'].iloc[math.ceil(test_set_size*6):math.ceil(test_set_size*6)+6].values[:,0:5]\n",
    "\n",
    "    #test key to compare to Y-hat\n",
    "    test_y = test_set.iloc[0]['data'].iloc[math.ceil(test_set_size*6):math.ceil(test_set_size*6)+6].values[:,5]\n",
    "    test_item = test_item.append({'id':item_id, 'name': test_set.iloc[0]['name'], 'train_x':train_x, 'train_y':train_y, 'test_x':test_x, 'test_y': test_y}, ignore_index=True)\n",
    "    print(\"Record completed at test_item position [\" + str(h) + \"]\")\n",
    "    h += 1\n",
    "    #TODO: Get this all in a for loop to iterate over multiple entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the creation of a neural network\n",
    "nn = NeuralNetwork()\n",
    "nn.generateInitialNetwork([1,1,1])\n",
    "nn.calculateNodeActivations()\n",
    "print(nn.outputNode.getActivation())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=\"+3\"><b>Testing of network size:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras.backend as K\n",
    "train_x = test_item.iloc[2]['train_x'] # test data. not real\n",
    " \n",
    "def create_model(layesr,activation):\n",
    "    model = Sequential()\n",
    "    for i, nodes in enumerate(layers):\n",
    "        if i==0:\n",
    "            model.add(Dense(nodes,input_dim=train_x.shape[1]))\n",
    "            model.add(Activation(activation))\n",
    "        else: \n",
    "            model.add(Dense(nodes))\n",
    "            model.add(Activation(activation))\n",
    "    model.add(Dense(1)) #Note: no activations present beyond this point\n",
    "\n",
    "    model.compile(optimizer='adadelta', loss='mse')\n",
    "    return model\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = [[16], [4,2], [4], [16,4]]\n",
    "activations = [tanh, relu]\n",
    "param_grid = dict(layers=layers, activation=activations, batch_size = [42, 180], epochs[6])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item List generated by item number:\n",
      "   id          name\n",
      "0   2    Cannonball\n",
      "1   6   Cannon base\n",
      "2   8  Cannon stand\n",
      "Querying API for Cannonball data. Item Id: 2\n",
      "Retrying...\n",
      "Querying API for Cannon base data. Item Id: 6\n",
      "Retrying...\n",
      "Querying API for Cannon stand data. Item Id: 8\n",
      "Item retrieval complete!\n",
      "Formatting item record dates...\n",
      "Creating Test and Training Datasets...\n",
      "Creating Datasets for item ID: 2\n",
      "The shape of the Cannonball input array is: (180, 6)\n",
      "Record completed at test_item position [0]\n",
      "Creating Datasets for item ID: 6\n",
      "The shape of the Cannon base input array is: (180, 6)\n",
      "Record completed at test_item position [1]\n",
      "Creating Datasets for item ID: 8\n",
      "The shape of the Cannon stand input array is: (180, 6)\n",
      "Record completed at test_item position [2]\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from neural_network import NeuralNetwork\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.models import Sequential\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_item_records_from_summary():\n",
    "    \"\"\" Generate item records using the summary.json link from the OSBuddy API \"\"\"\n",
    "\n",
    "    df = pd.read_json(path_or_buf='https://rsbuddy.com/exchange/summary.json',orient='index', convert_axes=True)\n",
    "    df = df[['id','name','buy_average','buy_quantity','sell_average','sell_quantity','overall_average','overall_quantity']]\n",
    "    data = df.sort_values(by=['id']).reset_index()\n",
    "    data = data.drop(labels='index',axis=1)\n",
    "\n",
    "    #Output item id/name pairs to a csv file\n",
    "    item_key = data[['id', 'name']]\n",
    "    file_name = './item_key.csv'\n",
    "    item_key.to_csv(path_or_buf=file_name, columns=('id','name'), index=False)\n",
    "\n",
    "\n",
    "def generate_input_data_by_item_number(start_num, num_items): \n",
    "    \"\"\"Pull items by range of Index values\"\"\"\n",
    "    file_name = './item_key.csv'\n",
    "    all_items = pd.read_csv(file_name, skiprows=[])\n",
    "    items = all_items[start_num: start_num + num_items]\n",
    "    print(\"Item List generated by item number:\")\n",
    "    print(items)\n",
    "    return items\n",
    "\n",
    "def generate_input_data_by_item_name(names):  #Pull a specific items by name\n",
    "    \"\"\"Pull specific items by name\"\"\"\n",
    "    file_name = './item_key.csv'\n",
    "    data = pd.read_csv(file_name, skiprows=[])\n",
    "    items = pd.DataFrame()\n",
    "    for name in names:\n",
    "        items = items.append(data.loc[data['name'] == name])\n",
    "    items = items.reset_index().drop(labels='index',axis=1)\n",
    "    print(\"Item List generated by name:\")\n",
    "    print(items)\n",
    "    return items\n",
    "\n",
    "def get_item_records_from_url(input_data, test_set_size):\n",
    "    \"\"\" Iterate through all items, grab data from api and append to dataframe \"\"\"\n",
    "    i = 0\n",
    "    item_lookup_failed = False\n",
    "    item_records = pd.DataFrame(columns=['id', 'name', 'data'])\n",
    "    while(i<input_data['id'].count()):\n",
    "        key = input_data.iloc[i]['id']\n",
    "        name = input_data.iloc[i]['name']\n",
    "        #Print item information only on first data retrieval attempt \n",
    "        if not item_lookup_failed:\n",
    "            print('Querying API for ' + name + ' data. ' + 'Item Id: ' + str(key))\n",
    "        try:\n",
    "            #Attempt to get data from API, retry if HTTP error\n",
    "            url = f'https://api.rsbuddy.com/grandExchange?a=graph&g=240&start=1474615279000&i={key}'\n",
    "            temp_df = pd.DataFrame()\n",
    "            temp_df = pd.read_json(path_or_buf=url,orient='records', convert_axes=False)\n",
    "            item_records = item_records.append({'id':key, 'name': name, 'data':temp_df}, ignore_index=True)\n",
    "            i+=1\n",
    "            item_lookup_failed = False\n",
    "        except:\n",
    "            print(\"Retrying...\")\n",
    "            time.sleep(1) #Avoid getting blacklisted by API\n",
    "            item_lookup_failed = True\n",
    "    print('Item retrieval complete!')\n",
    "\n",
    "    #Add correct formatting to item record dates/times\n",
    "    item_records = format_item_record_dates(item_records)\n",
    "\n",
    "    return item_records\n",
    "\n",
    "def format_item_record_dates(item_records):\n",
    "    \"\"\" Converts timestamp to Unix seconds, and tacks on formatted date field \"\"\"\n",
    "\n",
    "    print(\"Formatting item record dates...\")\n",
    "    for _, row in item_records.iterrows():\n",
    "        row['data'] = row['data'][['ts','buyingPrice','buyingCompleted','sellingPrice','sellingCompleted', 'overallPrice','overallCompleted']]\n",
    "        row['data'] = row['data'].sort_values(by=['ts'],ascending=1).reset_index()\n",
    "        for ind, r in row['data'].iterrows():\n",
    "            r['ts'] = r['ts']/1000\n",
    "            row['data'].loc[ind, 'ts'] = int(r['ts'])\n",
    "            row['data'].loc[ind, 'date'] = datetime.datetime.fromtimestamp(r['ts']).isoformat()\n",
    "        row['data'] = row['data'].drop(labels='index',axis=1)\n",
    "    return item_records\n",
    "\n",
    "def generate_train_and_test_deltas(item_records):\n",
    "    \"\"\" \n",
    "        Iterate through each item record and generate percentage deltas for buy price, \n",
    "        buy completed, sell price, sell completed, overall price, and overall completed\n",
    "        compared to last sample period. \n",
    "    \"\"\"\n",
    "    for _, row in item_records.iterrows():\n",
    "        for col_name in ['buyingPricePer', 'buyingCompletedPer', 'sellingPricePer', \n",
    "                        'sellingCompletedPer', 'overallPricePer', 'overallCompletedPer']:\n",
    "            series_length = len(row['data']['ts'])\n",
    "            row['data'][col_name] = pd.Series(np.random.randn(series_length), index=row['data'].index)\n",
    "        #Each row represents and item, which has an associated dataframe \n",
    "        for index, _ in row['data'].iterrows():\n",
    "            #Use data from last row to calc percentage change\n",
    "            for col_name in ['buyingPrice', 'buyingCompleted', 'sellingPrice', 'sellingCompleted', \n",
    "                            'overallPrice', 'overallCompleted']:\n",
    "                if(index == 0):\n",
    "                    row['data'].loc[index, col_name + 'Per'] = 0\n",
    "                else:\n",
    "                    row['data'].loc[index, col_name + 'Per'] = calculate_percentage_change(row['data'], col_name, index)\n",
    "                \n",
    "                \n",
    "            \n",
    "def calculate_percentage_change(dataframe, columnName, index):\n",
    "    \"\"\"Calculate delta from last time stamp\"\"\"\n",
    "    return (dataframe.loc[index, columnName] - dataframe.loc[index-1, columnName])/dataframe.loc[index-1, columnName]\n",
    "\n",
    "\n",
    "def createTrainAndTestSet(item_records):\n",
    "    print(\"Creating Test and Training Datasets...\")\n",
    "    #create test_item object to log training and test sets\n",
    "    test_item = pd.DataFrame(columns=['id', 'name', 'train_val', 'test_val', 'train_per', 'test_per', 'pred_per'])\n",
    "    record_index = 0\n",
    "    #iterate through the urlquery results to generate training sets\n",
    "    while(record_index < item_records['id'].count()):  \n",
    "        item_id = item_records.iloc[record_index]['id'] \n",
    "        print(\"Creating Datasets for item ID: \" + str(item_id))\n",
    "        \n",
    "        #takes the item_id and generates the test data for the specified parameters as an array\n",
    "        test_set = item_records.loc[item_records['id'] == item_id]\n",
    "        \n",
    "        #for column key, uncomment this line below:\n",
    "        #train_x_headers = list(urlquery.iloc[0]['data'].columns.values[0:6])\n",
    "        #print(train_x_headers)\n",
    "\n",
    "        #create training datasets\n",
    "        train_val = test_set.iloc[0]['data'].iloc[0:math.ceil(test_set_size*6)].values[:,0:6]\n",
    "        print (\"The shape of the \" + test_set.iloc[0]['name'] +  \" input array is: \"+ str(train_val.shape))\n",
    "\n",
    "        #create test set to estimate next 6 prices (next day of prices)\n",
    "        test_val = test_set.iloc[0]['data'].iloc[math.ceil(test_set_size*6):math.ceil(test_set_size*6)+6].values[:,0:6]\n",
    "\n",
    "        #Generate deltas\n",
    "        generate_train_and_test_deltas(item_records)\n",
    "\n",
    "        #test key to compare to Y-hat\n",
    "        test_item = 0\n",
    "        #test_item = test_item.append({'id':item_id, 'name': test_set.iloc[0]['name'], 'train_val': train_val, 'test_val': test_val, 'train_per': train_per, 'test_per': test_per}, ignore_index=True)\n",
    "        print(\"Record completed at test_item position [\" + str(record_index) + \"]\")\n",
    "        record_index += 1\n",
    "\n",
    "    return test_item\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #Get a list of all items\n",
    "    generate_item_records_from_summary()\n",
    "\n",
    "    #Pull items by range of Index values\n",
    "    input_data = generate_input_data_by_item_number(0,3)\n",
    "    \n",
    "    #Pull specific items by name\n",
    "    #input_data = generate_input_data_by_item_name(['Armadyl godsword','Fire rune', 'Soft clay'])\n",
    "\n",
    "    #In Days - define the number of epochs to include in the test data\n",
    "    test_set_size = 30\n",
    "    item_records = get_item_records_from_url(input_data, test_set_size)\n",
    "\n",
    "    #Create test_item object to log training and test sets\n",
    "    test_item = createTrainAndTestSet(item_records)\n",
    "\n",
    "    #Test creation of Neural Net\n",
    "    nn = NeuralNetwork()\n",
    "    nn.generateInitialNetwork([1,1])\n",
    "    nn.calculateNodeActivations()\n",
    "    print(nn.outputNode.getActivation())\n",
    "\n",
    "    #TODO: Create Year time stamp for seasonality\n",
    "    #TODO: Create time from today time stamp\n",
    "    #TODO: Create training and test sets\n",
    "    #TODO: Test model size\n",
    "\n",
    "\n",
    "    #random shit\n",
    "    #import keras.backend as K\n",
    "    \"\"\"\n",
    "    train_val = test_item.iloc[2]['train_val'] # test data. not real\n",
    "    train_y = test_item.iloc[2]['train_y'] # test data. not real\n",
    "\n",
    "    \n",
    "    def create_model(layers,activation):\n",
    "        model = Sequential()\n",
    "        for i, nodes in enumerate(layers):\n",
    "            if i==0:\n",
    "                model.add(Dense(nodes,input_dim=train_val.shape[1]))\n",
    "                model.add(Activation(activation))\n",
    "            else: \n",
    "                model.add(Dense(nodes))\n",
    "                model.add(Activation(activation))\n",
    "        model.add(Dense(1)) #Note: no activations present beyond this point\n",
    "\n",
    "        model.compile(optimizer='adadelta', loss='mse')\n",
    "        return model\n",
    "\n",
    "    print('creating model')\n",
    "    model = KerasRegressor(build_fn=create_model, verbose = 0)\n",
    "    layers = [[16], [4,2], [4], [16,4]]\n",
    "    activations = ['tanh', 'relu']\n",
    "    param_grid = dict(layers=layers, activation=activations, batch_size = [42, 180], epochs=[6])\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error')\n",
    "\n",
    "    # grid_result = grid.fit(train_x, train_y) testing for network size \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_records.iloc[0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = pd.DataFrame(columns=['ts','tsFromCurrent', 'tsYtd'],data=item_records.iloc[0]['data']['ts'])\n",
    "timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_records.iloc[0]['data']['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = pd.DataFrame(columns=['ts','tsFromCurrent', 'tsYtd'],data=item_records.iloc[0]['data']['ts'])\n",
    "lastTs = timestamps['ts'][len(timestamps)-1]\n",
    "i=0\n",
    "while(i<len(timestamps)):\n",
    "    timestamps['tsFromCurrent'][i] = timestamps['ts'][i] - lastTs\n",
    "    i += 1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonathan Stav\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "3973",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-4fe140b0c8dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0munix\u001b[0m \u001b[1;32min\u001b[0m \u001b[0myears\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'unix'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtimestamps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0munix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0munix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mtimestamps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tsYtd'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimestamps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0munix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   2475\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2476\u001b[0m             return self._engine.get_value(s, k,\n\u001b[1;32m-> 2477\u001b[1;33m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[0;32m   2478\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2479\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'integer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'boolean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 3973"
     ]
    }
   ],
   "source": [
    "timestamps = pd.DataFrame(columns=['ts','tsFromCurrent', 'tsYtd'],data=item_records.iloc[0]['data']['ts'])\n",
    "years = pd.DataFrame(columns=['year', 'unix'])\n",
    "\n",
    "i = 0\n",
    "for year in range(1970,2050):\n",
    "    unix = (year - 1970) * 31557600\n",
    "    years.loc[i] = [year,unix]\n",
    "    i+=1\n",
    "\n",
    "i = 0\n",
    "for unixYear in years['unix']:\n",
    "    if timestamps['ts'][i]<unixYear:\n",
    "        while(timestamps['ts'][i]<unixYear):\n",
    "            timestamps['tsYtd'][i] = timestamps['ts'][i] - unixYear\n",
    "            i+=1\n",
    "    else:\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year        unix\n",
      "0   1970           0\n",
      "1   1971    31557600\n",
      "2   1972    63115200\n",
      "3   1973    94672800\n",
      "4   1974   126230400\n",
      "5   1975   157788000\n",
      "6   1976   189345600\n",
      "7   1977   220903200\n",
      "8   1978   252460800\n",
      "9   1979   284018400\n",
      "10  1980   315576000\n",
      "11  1981   347133600\n",
      "12  1982   378691200\n",
      "13  1983   410248800\n",
      "14  1984   441806400\n",
      "15  1985   473364000\n",
      "16  1986   504921600\n",
      "17  1987   536479200\n",
      "18  1988   568036800\n",
      "19  1989   599594400\n",
      "20  1990   631152000\n",
      "21  1991   662709600\n",
      "22  1992   694267200\n",
      "23  1993   725824800\n",
      "24  1994   757382400\n",
      "25  1995   788940000\n",
      "26  1996   820497600\n",
      "27  1997   852055200\n",
      "28  1998   883612800\n",
      "29  1999   915170400\n",
      "..   ...         ...\n",
      "50  2020  1577880000\n",
      "51  2021  1609437600\n",
      "52  2022  1640995200\n",
      "53  2023  1672552800\n",
      "54  2024  1704110400\n",
      "55  2025  1735668000\n",
      "56  2026  1767225600\n",
      "57  2027  1798783200\n",
      "58  2028  1830340800\n",
      "59  2029  1861898400\n",
      "60  2030  1893456000\n",
      "61  2031  1925013600\n",
      "62  2032  1956571200\n",
      "63  2033  1988128800\n",
      "64  2034  2019686400\n",
      "65  2035  2051244000\n",
      "66  2036  2082801600\n",
      "67  2037  2114359200\n",
      "68  2038  2145916800\n",
      "69  2039  2177474400\n",
      "70  2040  2209032000\n",
      "71  2041  2240589600\n",
      "72  2042  2272147200\n",
      "73  2043  2303704800\n",
      "74  2044  2335262400\n",
      "75  2045  2366820000\n",
      "76  2046  2398377600\n",
      "77  2047  2429935200\n",
      "78  2048  2461492800\n",
      "79  2049  2493050400\n",
      "\n",
      "[80 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
