{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Function Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import datetime\n",
    "import time\n",
    "import pytz\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from neural_network import NeuralNetwork\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.models import Sequential\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_item_records_from_summary():\n",
    "    \"\"\" \n",
    "        Generate item records using the summary.json link from the OSBuddy API\n",
    "    \"\"\"\n",
    "    df = pd.read_json(path_or_buf='https://rsbuddy.com/exchange/summary.json',orient='index', convert_axes=True)\n",
    "    df = df[['id','name','buy_average','buy_quantity','sell_average','sell_quantity','overall_average','overall_quantity']]\n",
    "    data = df.sort_values(by=['id']).reset_index()\n",
    "    data = data.drop(labels='index',axis=1)\n",
    "    \n",
    "    #Output item id/name pairs to a csv file\n",
    "    item_key = data[['id', 'name']]\n",
    "    file_name = './item_key.csv'\n",
    "    item_key.to_csv(path_or_buf=file_name, columns=('id','name'), index=False)\n",
    "\n",
    "\n",
    "def generate_input_data_by_item_number(start_num, num_items): \n",
    "    \"\"\"\n",
    "        Pull items by range of Index values\n",
    "        \n",
    "            @param start_num: the starting index \n",
    "            @param num_items: the number of items to generate data for, starting from\n",
    "                              the start_num\n",
    "    \"\"\"\n",
    "    file_name = './item_key.csv'\n",
    "    all_items = pd.read_csv(file_name, skiprows=[])\n",
    "    items = all_items[start_num: start_num + num_items]\n",
    "    return items\n",
    "\n",
    "def generate_input_data_by_item_name(names): \n",
    "    \"\"\"\n",
    "        Pull specific items by name\n",
    "        \n",
    "            @param names: a list containing the item names to generate data for\n",
    "    \"\"\"\n",
    "    file_name = './item_key.csv'\n",
    "    data = pd.read_csv(file_name, skiprows=[])\n",
    "    items = pd.DataFrame()\n",
    "    for name in names:\n",
    "        items = items.append(data.loc[data['name'] == name])\n",
    "    items = items.reset_index().drop(labels='index',axis=1)\n",
    "    return items\n",
    "\n",
    "def get_item_records_from_url(input_data, frequency):\n",
    "    \"\"\" \n",
    "        Iterate through all items, grab data from api and append to dataframe \n",
    "        \n",
    "            @param input_data: a list of items with their associated index\n",
    "            @param frequency: the period at which data is sampled, in minutes\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    item_lookup_failed = False\n",
    "    item_records = pd.DataFrame(columns=['id', 'name', 'data'])\n",
    "    while(i < input_data['id'].count()):\n",
    "        key = input_data.iloc[i]['id']\n",
    "        name = input_data.iloc[i]['name']\n",
    "        #Print item information only on first data retrieval attempt \n",
    "        if not item_lookup_failed:\n",
    "            print('Querying API for ' + name + ' data. ' + 'Item Id: ' + str(key))\n",
    "        try:\n",
    "            #Attempt to get data from API, retry if HTTP error\n",
    "            url = f'https://api.rsbuddy.com/grandExchange?a=graph&g={frequency}&start=1474615279000&i={key}'\n",
    "            temp_df = pd.DataFrame()\n",
    "            temp_df = pd.read_json(path_or_buf=url, orient='records', convert_axes=False)\n",
    "            item_records = item_records.append({'id':key, 'name': name, 'data':temp_df}, ignore_index=True)\n",
    "            item_lookup_failed = False\n",
    "            i+=1\n",
    "        except:\n",
    "            print(\"Retrying...\")\n",
    "            time.sleep(1) #Avoid getting blacklisted by API\n",
    "            item_lookup_failed = True\n",
    "            \n",
    "    print('Item retrieval complete!')\n",
    "\n",
    "    #Add correct formatting to item record dates/times\n",
    "    item_records = format_item_record_dates(item_records)\n",
    "\n",
    "    return item_records\n",
    "\n",
    "def format_item_record_dates(item_records):\n",
    "    \"\"\" \n",
    "        Converts timestamp to Unix seconds, tacks on formatted date field, and creates Unix \n",
    "        seconds from most recent datapoint, and Unix seconds position from Jan-1.\n",
    "        \n",
    "            @param item_records: the master item data structure\n",
    "    \"\"\"\n",
    "    #Get most recent timestamp in all the data (highest value) \n",
    "    most_recent_ts = 0\n",
    "    for row in item_records['data']:\n",
    "        max_ts = row['ts'].max()\n",
    "        if max_ts > most_recent_ts:\n",
    "            most_recent_ts = max_ts\n",
    "    \n",
    "    #Get the timestamp of Jan-1 for 2015, 2016, 2017, and 2018\n",
    "    jan1_timestamps = {}\n",
    "    time_adjust = 18000 #Seconds ahead of EST\n",
    "    for year in [2016, 2017, 2018]:\n",
    "        jan1 = datetime.date(year, 1, 1)\n",
    "        jan1_ts = time.mktime(jan1.timetuple())\n",
    "        jan1_timestamps[str(year)] = jan1_ts - time_adjust\n",
    "\n",
    "    for row in item_records['data']:\n",
    "        #Convert timestamp from milliseconds to seconds\n",
    "        row['ts'] = row['ts'] / 1000\n",
    "        #Append column to indicate time delta from most recent record in dataset\n",
    "        row['tsFromCurrent'] = most_recent_ts - row['ts']\n",
    "        #Append column that converts ts to datetime object\n",
    "        row['date'] = pd.to_datetime(row['ts'], unit='s')\n",
    "        row['year'] = row['date'].dt.strftime('%Y')\n",
    "        #Append a column representing the YTD seconds\n",
    "        row['tsYtd'] = row['ts'] - pd.Series([jan1_timestamps[year] for year in row['year']])\n",
    "        row.drop(columns=['year'], inplace=True)\n",
    "\n",
    "            \n",
    "    return item_records\n",
    "\n",
    "def generate_train_and_test_deltas(item_records):    \n",
    "    \"\"\"\n",
    "        Calculate percent difference from previous record for each relevant column. \n",
    "            \n",
    "            @param item_records: the master item data structure\n",
    "    \"\"\"\n",
    "    #Iterate through each item, and calculate percentages for their data\n",
    "    for _, row in item_records.iterrows():\n",
    "        #Create temp dataframe for percent changes\n",
    "        pct_change_df = row['data'][['buyingPrice', 'buyingCompleted', 'sellingPrice', \n",
    "                            'sellingCompleted', 'overallPrice', 'overallCompleted']].copy()\n",
    "        pct_change_df = pct_change_df.rename(index=int, \n",
    "                                         columns={'buyingPrice':'buyingPricePer', 'buyingCompleted':'buyingCompletedPer',\n",
    "                                        'sellingPrice':'sellingPricePer', 'sellingCompleted':'sellingCompletedPer',\n",
    "                                        'overallPrice':'overallPricePer', 'overallCompleted':'overallCompletedPer'})\n",
    "        #Calculate percent change\n",
    "        pct_change_df = pct_change_df.pct_change()\n",
    "        \n",
    "        #Join percent change df back to original dataframe\n",
    "        row['data'] = row['data'].join(pct_change_df)\n",
    "        \n",
    "def generate_moving_averages(item_records, columns, window):\n",
    "    \"\"\"\n",
    "        Calculate the moving average for each field with specified window size\n",
    "        \n",
    "            @param item_records: the master item data structure\n",
    "            @param columns: a list of which columns (names) we are calculating average for\n",
    "            @param window: the number of periods to average over -- e.g. 60 if doing 12 hour item records for 30 days\n",
    "    \"\"\"\n",
    "    #Iterate through each item and calculate the moving average for each field\n",
    "    for _, row in item_records.iterrows():\n",
    "        for col in columns:\n",
    "            ma_col_name = col + 'MA'\n",
    "            row['data'][ma_col_name] = row['data'][col].rolling(window=window).mean()\n",
    "            \n",
    "            #Drop all rows where MA is NaN\n",
    "            row['data'] = row['data'].dropna(subset=[ma_col_name])\n",
    "            \n",
    "def generate_z_scores(item_records, columns, outlier_thresh=3):\n",
    "    \"\"\"\n",
    "        Generates z-scores for passed columns and appends to dataframe\n",
    "        \n",
    "            @param item_records: the master item data structure\n",
    "            @param columns: the columns to calculate z-score of\n",
    "            @param outlier_thresh: the zscore threshold after which we will remove a row\n",
    "    \"\"\"\n",
    "    for _, row in item_records.iterrows():\n",
    "        #Iterate through each item and calculate z-scores for columns\n",
    "        zscore_df = row['data'][columns].apply(zscore)\n",
    "        #Rename columns\n",
    "        rename_cols ={'buyingPricePer':'buyingPricePerZScore', 'buyingCompletedPer':'buyingCompletedPerZScore',\n",
    "                                        'sellingPricePer':'sellingPricePerZScore', 'sellingCompletedPer':'sellingCompletedPerZScore',\n",
    "                                        'overallPricePer':'overallPricePerZScore', 'overallCompletedPer':'overallCompletedPerZScore'}\n",
    "        zscore_df.rename(columns=rename_cols, inplace=True)\n",
    "        #Join zscore df to original item df\n",
    "        row['data'] = row['data'].join(zscore_df)\n",
    "        #Remove any outliers\n",
    "        init_rows = row['data'].shape[0]\n",
    "        for col in columns:\n",
    "            col_name = col + 'ZScore'\n",
    "            row['data'] = row['data'][abs(row['data'][col_name]) < outlier_thresh]\n",
    "            \n",
    "        #Display rows removed\n",
    "        new_rows = row['data'].shape[0]\n",
    "        print('Removed {0} Outlier Rows for {1}'.format(init_rows - new_rows, row['name']))\n",
    "        \n",
    "        #Reset Index\n",
    "        row['data'].reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "def createTrainAndTestSet(item_records, start_stamp, epoch_size, periods_per_epoch_unit):\n",
    "    \"\"\"\n",
    "        Create the training and testing data sets for a particular epoch starting at start_stamp, and extending\n",
    "        epoch_size units into the future.\n",
    "        \n",
    "            @param item_records: master item data structure\n",
    "            @param start_stamp: the starting time stamp of this epoch\n",
    "            @param epoch_size: the epoch size in days\n",
    "            @param periods_per_epoch_unit: the number of periods ahead of the last training record\n",
    "                that we will use this training data to predict\n",
    "    \"\"\"\n",
    "    print(\"Creating Test and Training Datasets...\")\n",
    "    # Create dataframe to hold training/test data for the epoch\n",
    "    epoch_dataframe = pd.DataFrame(columns=['id', 'name', 'train_data', 'test_data', 'pred_data'])\n",
    "    record_index = 0\n",
    "    end_stamp = start_stamp + math.ceil(epoch_size * periods_per_epoch_unit) \n",
    "    \n",
    "    # Iterate through the each item in item_records\n",
    "    while(record_index < item_records['id'].count()): \n",
    "        #Get item id\n",
    "        item_id = item_records.iloc[record_index]['id'] \n",
    "        \n",
    "        # Find all item records where ID matchest item_id\n",
    "        test_set = item_records.loc[item_records['id'] == item_id]\n",
    "        \n",
    "        # The time stamp of the value we'll use this training data to predict\n",
    "        predict_stamp = end_stamp + periods_per_epoch_unit\n",
    "        \n",
    "        #Ensure that the value we are attempting to predict exists for testing purposes\n",
    "        if predict_stamp < len(test_set.iloc[0]['data']):\n",
    "            # Create training dataset\n",
    "            train_df = test_set.iloc[0]['data'].iloc[start_stamp:end_stamp]\n",
    "            train_data = train_df[train_df.columns[-12:]][1:]          \n",
    "            \n",
    "            # Get test value -- the price 6 periods from the end of the training set (24 hours) \n",
    "            price_col = 14\n",
    "            test_data = test_set.iloc[0]['data'].iloc[predict_stamp][price_col]\n",
    "\n",
    "            # Append this item's data to the overall dataset for this epoch\n",
    "            epoch_dataframe = epoch_dataframe.append({'id':item_id, 'name': test_set.iloc[0]['name'], 'train_data': train_data, 'test_data': test_data}, ignore_index=True)\n",
    "            \n",
    "        record_index += 1\n",
    "\n",
    "    return epoch_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------ GENERATE INPUT DATA ------#\n",
    "#Get a list of all items\n",
    "generate_item_records_from_summary()\n",
    "\n",
    "#Pull items by range of Index values\n",
    "#input_data = generate_input_data_by_item_number(0,3)\n",
    "\n",
    "#Pull specific items by name\n",
    "input_data = generate_input_data_by_item_name(['Leather','Dragon boots', 'Rune arrow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Item Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying API for Leather data. Item Id: 1741\n",
      "Retrying...\n",
      "Retrying...\n",
      "Retrying...\n",
      "Querying API for Dragon boots data. Item Id: 11840\n",
      "Retrying...\n",
      "Retrying...\n",
      "Querying API for Rune arrow data. Item Id: 892\n",
      "Retrying...\n",
      "Retrying...\n",
      "Item retrieval complete!\n"
     ]
    }
   ],
   "source": [
    "#----- GENERATE ITEM RECORDS -----#\n",
    "item_records = get_item_records_from_url(input_data, 240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Data to Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- CONVERT DATA TO DELTAS -----#\n",
    "generate_train_and_test_deltas(item_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate 30-Day Averages For Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------ CREATE 30 DAY AVERAGES FOR COLUMNS -----#\n",
    "columns = ['overallPricePer', 'overallCompletedPer', 'buyingPricePer', 'buyingCompletedPer',\\\n",
    "           'sellingPricePer', 'sellingCompletedPer']\n",
    "window = 180 #24/4 hour periods * 30 days = 180 window\n",
    "generate_moving_averages(item_records, columns, window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Z-Scores and Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 132 Outlier Rows for Leather\n",
      "Removed 53 Outlier Rows for Dragon boots\n",
      "Removed 69 Outlier Rows for Rune arrow\n"
     ]
    }
   ],
   "source": [
    "#----- Generate Z-Scores for Percentage columns and remove outliers -----#\n",
    "generate_z_scores(item_records, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Test and Training Datasets...\n",
      "Creating Test and Training Datasets...\n",
      "Creating Test and Training Datasets...\n",
      "Creating Test and Training Datasets...\n",
      "Creating Test and Training Datasets...\n",
      "Creating Test and Training Datasets...\n"
     ]
    }
   ],
   "source": [
    "#------- CREATE TRAINING AND TEST DATASETS -----#\n",
    "#In Days - define the number of epochs to include in the test data\n",
    "periods_per_epoch_unit = 6 #Each value is a 4 hour record\n",
    "start_stamp = 0\n",
    "epoch_size = 30\n",
    "num_epochs = 3\n",
    "\n",
    "#Create test_item object to log training and test sets\n",
    "test_train_set = pd.DataFrame(columns=['epoch_id', 'item_data'])\n",
    "\n",
    "#Calculate number of epochs required\n",
    "\n",
    "#Get item data for each epoch\n",
    "for i in range(num_epochs):\n",
    "    curr_epoch = pd.DataFrame(columns=['epoch_id', 'item_data'])\n",
    "    test_data = createTrainAndTestSet(item_records, start_stamp, epoch_size, periods_per_epoch_unit)\n",
    "    curr_epoch = curr_epoch.append([{'epoch_id': i, 'item_data': createTrainAndTestSet(item_records, start_stamp, epoch_size, periods_per_epoch_unit)}])\n",
    "    #print(curr_epoch)\n",
    "    test_train_set = test_train_set.append(curr_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras.backend as K\n",
    "train_x = test_item.iloc[2]['train_x'] # test data. not real\n",
    " \n",
    "def create_model(layesr,activation):\n",
    "    model = Sequential()\n",
    "    for i, nodes in enumerate(layers):\n",
    "        if i==0:\n",
    "            model.add(Dense(nodes,input_dim=train_x.shape[1]))\n",
    "            model.add(Activation(activation))\n",
    "        else: \n",
    "            model.add(Dense(nodes))\n",
    "            model.add(Activation(activation))\n",
    "    model.add(Dense(1)) #Note: no activations present beyond this point\n",
    "\n",
    "    model.compile(optimizer='adadelta', loss='mse')\n",
    "    return model\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [[16], [4,2], [4], [16,4]]\n",
    "activations = [tanh, relu]\n",
    "param_grid = dict(layers=layers, activation=activations, batch_size = [42, 180], epochs[6])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
