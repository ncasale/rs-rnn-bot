{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "- Ignore NaN values when calculating moving averages\n",
    "- Clean dfs that have column headers but don't have data\n",
    "- Create mechanism that will cut dataframe into hourly, 12 hour, daily, etc slices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Function Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "import pickle\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import datetime\n",
    "import time\n",
    "import pytz\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import sys\n",
    "\n",
    "\n",
    "def generate_item_lookup(file_name):\n",
    "    \"\"\"\n",
    "        Reads in item name/ids from file\n",
    "        \n",
    "            @param file_name: file containing item name/ids\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_name, skiprows=[])\n",
    "\n",
    "def get_item_records_from_pickle(file_name):\n",
    "    \"\"\"\n",
    "        This method loads data from the Moophan provided Pickle files instead of from \n",
    "        the deprecated RSBuddy API\n",
    "        \n",
    "            @param file_name: the name of the pickle file from which to load data\n",
    "    \"\"\"\n",
    "    # Open file and read into data structure\n",
    "    with open(file_name, 'rb') as row:\n",
    "        ge_data = pickle.load(row)\n",
    "        \n",
    "    # Get all item name/ids for lookup\n",
    "    item_lookup = generate_item_lookup('./item_key.csv')\n",
    "        \n",
    "    # Create data frame from loaded data by iterating through all contained items\n",
    "    item_records = pd.DataFrame(columns=['id', 'name', 'data'])\n",
    "    item_keys = list(ge_data.keys())\n",
    "    curr = 0\n",
    "    for key in item_keys:\n",
    "        #Get name of item\n",
    "        name = item_lookup.loc[item_lookup['id'] == int(key)]['name'].item()\n",
    "        sys.stdout.write('\\rProcessing Item: {0} / {1}'.format(curr, len(item_keys)))\n",
    "        sys.stdout.flush()\n",
    "        #Grab data\n",
    "        data = pd.DataFrame(ge_data[key])\n",
    "        item_records = item_records.append({'id':int(key), 'name': name, 'data':data}, ignore_index=True)\n",
    "        curr += 1\n",
    "        \n",
    "    # Print Complettion\n",
    "    completion_time = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    sys.stdout.write('\\r{0} -- Loading From Pickle Complete.'.format(completion_time))\n",
    "    sys.stdout.flush()\n",
    "        \n",
    "    return item_records\n",
    "\n",
    "def drop_empty_items(item_records):\n",
    "    \"\"\"\n",
    "        Will check to see if item data is empty, if so, drop item\n",
    "            \n",
    "            @param item_records: pandas dataframe representing dataset\n",
    "    \"\"\"\n",
    "    # Find all indices to drop\n",
    "    drop_indices = []\n",
    "    for idx, row in item_records.iterrows():\n",
    "        #Drop row if item data empty\n",
    "        if(row['data'].empty):\n",
    "            drop_indices.append(idx)\n",
    "        else:            \n",
    "            #Drop row if data doesn't have all columns\n",
    "            columns = set(['ts', 'buyingPrice', 'buyingCompleted', 'sellingPrice', 'sellingCompleted', 'overallPrice', 'overallCompleted'])\n",
    "            if not columns.issubset(set(row['data'].columns)):\n",
    "                drop_indices.append(idx)\n",
    "\n",
    "            # Drop row if data has no rows\n",
    "            if(row['data'].shape[0] == 0):\n",
    "                drop_indices.append(idx)\n",
    "              \n",
    "    #Make drop_indices unique\n",
    "    drop_indices = list(set(drop_indices)) \n",
    "    #Drop rows with empty data\n",
    "    item_records = item_records.drop(drop_indices)    \n",
    "    #Reset index\n",
    "    item_records.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Print Completion\n",
    "    print('Dropped {0} Rows from Dataset'.format(len(drop_indices)))\n",
    "    completion_time = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    print('{0} -- Data Cleaning Complete.'.format(completion_time))\n",
    "    \n",
    "    return item_records\n",
    "    \n",
    "\n",
    "def format_item_record_dates(item_records):\n",
    "    \"\"\" \n",
    "        Converts timestamp to Unix seconds, tacks on formatted date field, and creates Unix \n",
    "        seconds from most recent datapoint, and Unix seconds position from Jan-1.\n",
    "        \n",
    "            @param item_records: the master item data structure\n",
    "    \"\"\"\n",
    "    #Get most recent timestamp in all the data (highest value) \n",
    "    most_recent_ts = 0\n",
    "    for _, row in item_records.iterrows():\n",
    "        max_ts = row['data']['ts'].max()\n",
    "        if max_ts > most_recent_ts:\n",
    "            most_recent_ts = max_ts / 1000 #Convert to seconds\n",
    "    \n",
    "    #Get the timestamp of Jan-1 for 2015, 2016, 2017, and 2018\n",
    "    jan1_timestamps = {}\n",
    "    time_adjust = 18000 #Seconds ahead of EST\n",
    "    for year in [2015, 2016, 2017, 2018]:\n",
    "        jan1 = datetime.date(year, 1, 1)\n",
    "        jan1_ts = time.mktime(jan1.timetuple())\n",
    "        jan1_timestamps[str(year)] = jan1_ts - time_adjust\n",
    "        \n",
    "    for idx, row in item_records.iterrows():\n",
    "        #Convert timestamp from milliseconds to seconds\n",
    "        row['data']['ts'] = row['data']['ts'] / 1000\n",
    "        #Append column to indicate time delta from most recent record in dataset\n",
    "        row['data']['tsFromCurrent'] = most_recent_ts - row['data']['ts']\n",
    "        #Append column that converts ts to datetime object\n",
    "        row['data']['date'] = pd.to_datetime(row['data']['ts'], unit='s')\n",
    "        row['data']['year'] = row['data']['date'].dt.strftime('%Y')\n",
    "        #Append a column representing the YTD seconds\n",
    "        row['data']['tsYtd'] = row['data']['ts'] - pd.Series([jan1_timestamps[year] for year in row['data']['year']])\n",
    "        row['data'].drop(columns=['year'], inplace=True)\n",
    "        \n",
    "        sys.stdout.write('\\rProcessing Item: {0}/{1}'.format(idx, item_records.shape[0]))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    completion_time = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    sys.stdout.write('\\r{0} -- Date Formatting Complete.'.format(completion_time))\n",
    "    sys.stdout.flush()\n",
    "    print('')\n",
    "            \n",
    "    return item_records\n",
    "\n",
    "def fill_nans_with_ma(item_records):\n",
    "    '''\n",
    "        Fill NaN values in basic cols (buyingCompletd, sellingPrice, etc.) with rolling average of previous 5\n",
    "        entries\n",
    "        \n",
    "            @param item_records: dataframe containing all item data\n",
    "    '''\n",
    "    # Iterate through each item\n",
    "    for idx, row in item_records.iterrows():\n",
    "        # Progress \n",
    "        sys.stdout.write('\\rProcessing Item: {0}/{1}'.format(idx, item_records.shape[0]))\n",
    "        sys.stdout.flush()        \n",
    "        # Fill NaN values in columns below with rolling average of past 5 entries (excluding other NaNs)\n",
    "        cols_to_fill = ['buyingCompleted', 'buyingPrice', 'sellingCompleted', 'sellingPrice', 'overallCompleted', 'overallPrice']\n",
    "        row['data'][cols_to_fill] = row['data'][cols_to_fill].fillna(row['data'][cols_to_fill].rolling(window=6, min_periods=1).mean())\n",
    "        \n",
    "        # Drop any rows that were unable to be filled \n",
    "        row['data'] = row['data'].dropna()\n",
    "        \n",
    "    # Print complete \n",
    "    completion_time = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    sys.stdout.write('\\r{0} -- NaN Replacement Complete.'.format(completion_time))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    return item_records\n",
    "\n",
    "def generate_train_and_test_deltas(item_records):    \n",
    "    \"\"\"\n",
    "        Calculate percent difference from previous record for each relevant column. \n",
    "            \n",
    "            @param item_records: the master item data structure\n",
    "    \"\"\"\n",
    "    #Iterate through each item, and calculate percentages for their data\n",
    "    for idx, row in item_records.iterrows():\n",
    "        sys.stdout.write('\\rProcessing Item: {0}/{1}'.format(idx, item_records.shape[0]))\n",
    "        sys.stdout.flush()\n",
    "        #Create temp dataframe for percent changes\n",
    "        pct_change_df = row['data'][['buyingPrice', 'buyingCompleted', 'sellingPrice', 'sellingCompleted', 'overallPrice', 'overallCompleted']].copy()\n",
    "        pct_change_df = pct_change_df.rename(index=int, \n",
    "                                         columns={'buyingPrice':'buyingPricePer', 'buyingCompleted':'buyingCompletedPer',\n",
    "                                        'sellingPrice':'sellingPricePer', 'sellingCompleted':'sellingCompletedPer',\n",
    "                                        'overallPrice':'overallPricePer', 'overallCompleted':'overallCompletedPer'})\n",
    "        #Calculate percent change\n",
    "        pct_change_df = pct_change_df.pct_change()\n",
    "        \n",
    "        #Join pct_change_df to existing row\n",
    "        row['data'] = row['data'].join(pct_change_df)\n",
    "        \n",
    "    # Print Completion\n",
    "    completion_time = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    sys.stdout.write('\\r{0} -- Delta Creation Complete.'.format(completion_time))\n",
    "    sys.stdout.flush()\n",
    "        \n",
    "    return item_records\n",
    "        \n",
    "def get_next_period_overall_price(item_records):\n",
    "    \"\"\"\n",
    "        Append the next period's overall price to each entry\n",
    "        \n",
    "            @param item_records: the master item data structure\n",
    "    \"\"\"\n",
    "    #Iterate through each item in item_records\n",
    "    for idx, row in item_records.iterrows():\n",
    "        #Progress\n",
    "        sys.stdout.write('\\rProcessing Item: {0}/{1}'.format(idx, item_records.shape[0]))\n",
    "        sys.stdout.flush()\n",
    "        #Append new column for next period value\n",
    "        new_col_name = 'overallPriceNext'\n",
    "        row['data'][new_col_name] = row['data']['overallPricePer'].shift(-1)\n",
    "        #Drop any rows that didn't have a next period price\n",
    "        row['data'] = row['data'].dropna(subset=[new_col_name])\n",
    "        \n",
    "    # Print completion\n",
    "    completion_time = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    sys.stdout.write('\\r{0} -- Next Period Price Calculation Complete.'.format(completion_time))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    return item_records\n",
    "        \n",
    "        \n",
    "def generate_moving_averages(item_records, columns, window):\n",
    "    \"\"\"\n",
    "        Calculate the moving average for each field with specified window size\n",
    "        \n",
    "            @param item_records: the master item data structure\n",
    "            @param columns: a list of which columns (names) we are calculating average for\n",
    "            @param window: the number of periods to average over -- e.g. 60 if doing 12 hour item records for 30 days\n",
    "    \"\"\"\n",
    "    #Iterate through each item and calculate the moving average for each field\n",
    "    for idx, row in item_records.iterrows():\n",
    "        #Progress\n",
    "        sys.stdout.write('\\rProcessing Item: {0}/{1}'.format(idx, item_records.shape[0]))\n",
    "        sys.stdout.flush()\n",
    "        for col in columns:\n",
    "            ma_col_name = col + 'MA'\n",
    "            row['data'][ma_col_name] = row['data'][col].rolling(window=window, min_periods=int(window/2)).mean()\n",
    "            \n",
    "            #Drop all rows where MA is NaN\n",
    "            row['data'] = row['data'].dropna(subset=[ma_col_name])\n",
    "            \n",
    "    # Print completion\n",
    "    completion_time = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    sys.stdout.write('\\r{0} -- Moving Average Calculation Complete.'.format(completion_time))\n",
    "    sys.stdout.flush()\n",
    "            \n",
    "    return item_records\n",
    "            \n",
    "def generate_z_scores(item_records, columns, outlier_thresh=3):\n",
    "    \"\"\"\n",
    "        Generates z-scores for passed columns and appends to dataframe\n",
    "        \n",
    "            @param item_records: the master item data structure\n",
    "            @param columns: the columns to calculate z-score of\n",
    "            @param outlier_thresh: the zscore threshold after which we will remove a row\n",
    "    \"\"\"\n",
    "    for idx, row in item_records.iterrows():\n",
    "        #Iterate through each item and calculate z-scores for columns\n",
    "        zscore_df = row['data'][columns].apply(zscore)\n",
    "        #Rename columns\n",
    "        rename_cols ={'buyingPricePer':'buyingPricePerZScore', 'buyingCompletedPer':'buyingCompletedPerZScore',\n",
    "                                        'sellingPricePer':'sellingPricePerZScore', 'sellingCompletedPer':'sellingCompletedPerZScore',\n",
    "                                        'overallPricePer':'overallPricePerZScore', 'overallCompletedPer':'overallCompletedPerZScore'}\n",
    "        zscore_df.rename(columns=rename_cols, inplace=True)\n",
    "        #Join zscore df to original item df\n",
    "        row['data'] = row['data'].join(zscore_df)\n",
    "        #Remove any outliers\n",
    "        init_rows = row['data'].shape[0]\n",
    "        for col in columns:\n",
    "            col_name = col + 'ZScore'\n",
    "            row['data'] = row['data'][abs(row['data'][col_name]) < outlier_thresh]\n",
    "            \n",
    "        #Display rows removed\n",
    "        new_rows = row['data'].shape[0]\n",
    "        sys.stdout.write('\\rRemoved {0} Outlier Rows for {1}'.format(init_rows - new_rows, row['name']))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        #Reset Index\n",
    "        row['data'].reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    # Print completion\n",
    "    completion_time = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    sys.stdout.write('\\r{0} -- Z-Score Calculation Complete.'.format(completion_time))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    return item_records\n",
    "        \n",
    "def remove_excess_columns(item_records):\n",
    "    '''\n",
    "        Removes columns that aren't necessary to train or evaluate the ANN\n",
    "        \n",
    "            @param item_records: the master item data structure\n",
    "    '''\n",
    "    cols_to_keep = ['buyingPricePer', 'sellingPricePer', 'overallPricePer',\n",
    "                    'buyingCompletedPer', 'sellingCompletedPer', 'overallCompletedPer',\n",
    "                    'overallPriceNext', 'tsFromCurrent', 'tsYtd',\n",
    "                    'buyingPricePerMA', 'sellingPricePerMA', 'overallPricePerMA',\n",
    "                    'buyingCompletedPerMA', 'sellingPricePerMA', 'overallPricePerMA']\n",
    "    #Iterate through each item and drop excess cols\n",
    "    for idx, row in item_records.iterrows():\n",
    "        #Progress\n",
    "        sys.stdout.write('\\rProcessing Item: {0}/{1}'.format(idx, item_records.shape[0]))\n",
    "        sys.stdout.flush()\n",
    "        row['data'] = row['data'][cols_to_keep]\n",
    "        \n",
    "    # Print completion\n",
    "    completion_time = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    sys.stdout.write('\\r{0} -- Column Removal Complete.'.format(completion_time))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def append_names_to_item_records(item_records):\n",
    "    \"\"\"\n",
    "        Appends the item name to each row in item record data\n",
    "        \n",
    "            @param item_record: the master item data structure\n",
    "    \"\"\"\n",
    "    for idx, row in item_records.iterrows():\n",
    "        #Progress\n",
    "        sys.stdout.write('\\rProcessing Item: {0}/{1}'.format(idx, item_records.shape[0]))\n",
    "        sys.stdout.flush()\n",
    "        row['data']['item_name'] = row['name']\n",
    "        \n",
    "    # Print completion\n",
    "    completion_time = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    sys.stdout.write('\\r{0} -- Name Appension Complete.'.format(completion_time))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def aggregate_cleaned_item_data(item_records):\n",
    "    \"\"\"\n",
    "        Takes in cleaned master item data structure and returns a dataframe with all\n",
    "        item data aggregated\n",
    "        \n",
    "            @param item_records: the master item data structure (cleaned)\n",
    "            @returns: a new dataframe with aggregated item data\n",
    "    \"\"\"\n",
    "    #Apply item names to all item records\n",
    "    append_names_to_item_records(item_records)\n",
    "    aggregate_df = pd.DataFrame()\n",
    "    #Iterate through each item and append to aggregate df\n",
    "    for idx, row in item_records.iterrows():\n",
    "        #Progress\n",
    "        sys.stdout.write('\\rAggregating Item: {0}/{1}'.format(idx, item_records.shape[0]))\n",
    "        sys.stdout.flush()\n",
    "        aggregate_df = aggregate_df.append(row['data'], ignore_index=True)\n",
    "    \n",
    "    #Reset index in place\n",
    "    aggregate_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Print completion\n",
    "    completion_time = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    sys.stdout.write('\\r{0} -- Item Aggregation Complete.'.format(completion_time))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    return aggregate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Item Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:22:51 -- Loading From Pickle Complete."
     ]
    }
   ],
   "source": [
    "#----- GENERATE ITEM RECORDS -----#\n",
    "item_records = get_item_records_from_pickle('ge_data_1.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 9 Rows from Dataset\n",
      "17:29:44 -- Data Cleaning Complete.\n",
      "17:33:16 -- Date Formatting Complete.\n",
      "17:33:30 -- NaN Replacement Complete."
     ]
    }
   ],
   "source": [
    "item_records = drop_empty_items(item_records)\n",
    "item_records = format_item_record_dates(item_records)\n",
    "item_records = fill_nans_with_ma(item_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Data to Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:56:58 -- Delta Creation Complete."
     ]
    }
   ],
   "source": [
    "#----- CONVERT DATA TO DELTAS -----#\n",
    "item_records = generate_train_and_test_deltas(item_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Period's Overall Price for Each Entry\n",
    "These next period values are what we will be attempting to predict with the ANN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:57:08 -- Next Period Price Calculation Complete."
     ]
    }
   ],
   "source": [
    "#----- GET NEXT PERIOD OVERALL PRICE FOR EACH ENTRY -----#\n",
    "item_records = get_next_period_overall_price(item_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate 30-Day Averages For Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:57:53 -- Moving Average Calculation Complete."
     ]
    }
   ],
   "source": [
    "#------ CREATE 30 DAY AVERAGES FOR COLUMNS -----#\n",
    "columns = ['overallPricePer', 'overallCompletedPer', 'buyingPricePer', 'buyingCompletedPer',\\\n",
    "           'sellingPricePer', 'sellingCompletedPer']\n",
    "window = 1440 #24/.5 hour periods * 30 days = 1440 window\n",
    "item_records = generate_moving_averages(item_records, columns, window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Z-Scores and Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 92 Rows from Dataset\n",
      "18:09:50 -- Data Cleaning Complete.\n",
      "Removed 54350 Outlier Rows for Priest gowndetet))unf)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nate\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\_methods.py:117: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      "c:\\users\\nate\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\stats\\stats.py:2253: RuntimeWarning: invalid value encountered in subtract\n",
      "  return (a - mns) / sstd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:10:17 -- Z-Score Calculation Complete.ts (e)ompss"
     ]
    }
   ],
   "source": [
    "#----- Generate Z-Scores for Percentage columns and remove outliers -----#\n",
    "item_records = drop_empty_items(item_records)\n",
    "item_records = generate_z_scores(item_records, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Columns Not Required for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:10:31 -- Column Removal Complete."
     ]
    }
   ],
   "source": [
    "remove_excess_columns(item_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate all Cleaned Item Data\n",
    "\n",
    "We set item_records equal to the aggregation to free up memory that would be wasted if they were both populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:14:11 -- Item Aggregation Complete."
     ]
    }
   ],
   "source": [
    "item_records = aggregate_cleaned_item_data(item_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aggregate_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
